# -*- coding: utf-8 -*-
"""Exercise_ABN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13KJmWMBS1QUPzSXVaMjC8RElvAZ32piH

## ABN Exercise for KommatiPara
"""

!pip install pyspark

from pyspark.sql import SparkSession

spark = SparkSession \
    .builder \
    .getOrCreate()

from pyspark.sql.functions import col
import logging
import os

logging.basicConfig(filename='/application.log', encoding='utf-8', level=logging.DEBUG)

# Download the datasets from the Github repository
logging.info('Reading datasets from Github')
!wget --no-check-certificate \
    https://raw.githubusercontent.com/rickjhee/KommatiParaABN/main/dataset_one.csv \
    -O /tmp/dataset_one.csv

!wget --no-check-certificate \
    https://raw.githubusercontent.com/rickjhee/KommatiParaABN/main/dataset_two.csv \
    -O /tmp/dataset_two.csv

def perform_data_operations(dataset_1, dataset_2):
  """
  Performs data operations based on several requirements 

  Inputs: dataset_1, dataset_2
  Outputs: joined dataset
  """
  # Requirement 2: Only use clients from the UK or NL
  dataset_1 = dataset_1.filter( (dataset_1.country  == "United Kingdom") | (dataset_1.country == "Netherlands") )

  # Requirement 3: Remove personal identifiable information from the first dataset, excluding emails.
  dataset_1 = dataset_1.drop('first_name', 'last_name')

  #Requirement 4: Remove credit card number from the second dataset.
  dataset_2 = dataset_2.drop('cc_n')

  # Requirement 5: Data should be joined using the id field.
  joined = dataset_1.join(dataset_2, on='id')

  # Requirement 6:	Rename the columns for the easier readability to the business users 
  rename_dict = {'id': 'client_identifier', 'btc_a': 'bitcoin_address', 'cc_t': 'credit_card_type'}

  joined = joined.select([col(c).alias(rename_dict.get(c, c)) for c in joined.columns]) 

  return joined

df1 = spark.read.csv("/tmp/dataset_one.csv", header=True)
df2 = spark.read.csv("/tmp/dataset_two.csv", header=True)
# Displays the content of the DataFrame to stdout

df1.show()
df2.show()

joined = perform_data_operations(df1, df2)

joined.show()

#Requirement 8: Save the output in a client_data directory in the root directory of the project.
os.makedirs('/tmp/client_data', exist_ok=True)
logging.info('Saving joined dataset')
joined.toPandas().to_csv('/tmp/client_data/client_data.csv')
